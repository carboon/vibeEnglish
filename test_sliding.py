"""
æ”¹è¿›ç‰ˆï¼šä½¿ç”¨æ»‘åŠ¨çª—å£æç¤ºè¯æå‡å™äº‹è¿è´¯æ€§
"""

import base64
import json
import os
import spacy
from wordfreq import zipf_frequency
from zai import ZhipuAiClient

# åˆå§‹åŒ–
client = ZhipuAiClient(api_key="9c6603c2f1ee4a94b900f219f165d976.CYox1I8usvuEqM82")

# åŠ è½½ spacy æ¨¡å‹
try:
    nlp = spacy.load("en_core_web_sm")
except OSError:
    print("Downloading spacy model...")
    os.system("python -m spacy download en_core_web_sm")
    nlp = spacy.load("en_core_web_sm")


def encode_image(image_path):
    """å°†å›¾ç‰‡ç¼–ç ä¸º base64"""
    try:
        with open(image_path, "rb") as image_file:
            return base64.b64encode(image_file.read()).decode('utf-8')
    except Exception as e:
        print(f"Error encoding image: {e}")
        return None


def get_word_level(zipf_val):
    """æ ¹æ® Zipf é¢‘ç‡å€¼è¿”å› CEFR ç­‰çº§"""
    if zipf_val < 3.0:
        return "C1/C2"
    elif zipf_val < 4.0:
        return "B2"
    elif zipf_val < 4.5:
        return "B1/B2"
    else:
        return "A1/A2"


def linguistic_post_process(llm_result, previous_context=""):
    """
    åå¤„ç†é€»è¾‘ï¼šè¯æ±‡åˆ†çº§ã€æ ¸å¿ƒè¯é€‰æ‹©
    """
    if not llm_result or "video_narrative" not in llm_result:
        print("Warning: Invalid or empty LLM result")
        return llm_result

    # æå–ç‰©ä½“åæ ‡æ˜ å°„
    grounding_map = {
        obj["label"].lower(): obj["boxes"]
        for obj in llm_result.get("detected_objects", [])
    }

    refined_narrative = []
    for entry in llm_result["video_narrative"]:
        sentence = entry.get("sentence", "")
        if not sentence:
            continue

        doc = nlp(sentence)

        candidates = []
        for token in doc:
            if not token.is_stop and not token.is_punct and len(token.text) > 2:
                word_lemma = token.lemma_.lower()
                zipf_val = zipf_frequency(word_lemma, 'en')

                if 2.5 < zipf_val < 5.5:
                    word_info = {
                        "word": token.text,
                        "lemma": word_lemma,
                        "level": get_word_level(zipf_val),
                        "frequency": f"Zipf: {round(zipf_val, 2)}",
                        "pos": token.pos_
                    }

                    if token.pos_ in ["NOUN", "PROPN"]:
                        coords = grounding_map.get(word_lemma) or grounding_map.get(token.text.lower())
                        word_info["coordinates"] = coords if coords else []

                    candidates.append(word_info)

        candidates.sort(key=lambda x: zipf_frequency(x['lemma'], 'en'))
        entry["advanced_vocabulary"] = candidates
        entry["core_word"] = candidates[0]["word"] if candidates else ""
        entry["vocabulary_count"] = len(candidates)

        # æ·»åŠ ä¸Šä¸‹æ–‡ä¿¡æ¯
        if previous_context:
            entry["context_continuity"] = {
                "previous_sentence": previous_context
            }

        refined_narrative.append(entry)

    return {"video_narrative": refined_narrative}


def parse_llm_response(response_text):
    """è§£æ LLM è¿”å›çš„å†…å®¹ï¼Œæå– JSON"""
    try:
        if "```json" in response_text:
            json_str = response_text.split("```json")[1].split("```")[0].strip()
        elif "```" in response_text:
            json_str = response_text.split("```")[1].split("```")[0].strip()
        else:
            json_str = response_text.strip()

        return json.loads(json_str)
    except json.JSONDecodeError as e:
        print(f"JSON decode error: {e}")
        print(f"Raw response: {response_text[:500]}...")
        return None


def analyze_video_frames_with_sliding_window(image_paths, max_retries=2):
    """
    ä½¿ç”¨æ»‘åŠ¨çª—å£æç¤ºè¯åˆ†æè§†é¢‘å¸§ï¼Œæå‡å™äº‹è¿è´¯æ€§

    Args:
        image_paths: å›¾ç‰‡è·¯å¾„åˆ—è¡¨
        max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°

    Returns:
        åŒ…å«è¿è´¯å™äº‹çš„ JSON ç»“æœ
    """
    all_narratives = []
    previous_sentence = ""

    print(f"\nğŸ”„ ä½¿ç”¨æ»‘åŠ¨çª—å£æ¨¡å¼åˆ†æ {len(image_paths)} å¸§")
    print("=" * 60)

    for i, image_path in enumerate(image_paths):
        print(f"\nğŸ“· åˆ†æç¬¬ {i+1}/{len(image_paths)} å¸§")

        # æ„å»ºå¸¦æœ‰ä¸Šä¸‹æ–‡çš„ Prompt
        context_part = ""
        if previous_sentence:
            context_part = f"""
### Context Continuity
The previous frame was described as: "{previous_sentence}"

### Instruction
Your description should:
1. Continue naturally from the previous description
2. Maintain narrative coherence and flow
3. Describe what changed or what's new in this scene
4. Use consistent terminology (e.g., don't switch between "rabbit" and "bunny")
"""

        prompt_text = f"""
### Role
Expert English Teacher & Visual Analyst specializing in narrative continuity.

### Task
Write a natural, descriptive narrative sentence for this image.

{context_part}
### Guidelines
- Use descriptive, academic English (15-25 words)
- Include advanced vocabulary (B2+ level words)
- Maintain narrative flow with previous frames
- Be accurate with object detection
- Output ONLY valid JSON

### Output Format (Strict JSON ONLY)
{{
  "video_narrative": [
    {{
      "frame_index": {i},
      "timestamp": "{i*2:02d}:{(i*2)%60:02d}",
      "sentence": "Your descriptive sentence here."
    }}
  ]{"," if previous_sentence else ""}
  "detected_objects": [
    {{
      "label": "noun from the sentence",
      "boxes": [[ymin, xmin, ymax, xmax]]
    }}
  ]
}}
"""

        content_list = [{"type": "text", "text": prompt_text}]
        base64_data = encode_image(image_path)
        if base64_data:
            content_list.append({
                "type": "image_url",
                "image_url": {"url": f"data:image/jpeg;base64,{base64_data}"}
            })

        # è°ƒç”¨ APIï¼Œå¸¦é‡è¯•
        for attempt in range(max_retries):
            try:
                response = client.chat.completions.create(
                    model="glm-4.6v-flash",
                    messages=[{"role": "user", "content": content_list}],
                    thinking={"type": "enabled"}
                )

                full_response = response.choices[0].message.content
                raw_json = parse_llm_response(full_response)

                if raw_json:
                    # åå¤„ç†
                    processed = linguistic_post_process(raw_json, previous_sentence)
                    all_narratives.extend(processed["video_narrative"])

                    # æ›´æ–°ä¸Šä¸‹æ–‡
                    if processed["video_narrative"]:
                        previous_sentence = processed["video_narrative"][0]["sentence"]
                        print(f"âœ“ å¥å­: {previous_sentence}")
                        print(f"  æ ¸å¿ƒè¯: {processed['video_narrative'][0].get('core_word', '')}")

                    break
                else:
                    print(f"Attempt {attempt + 1}: Failed to parse JSON")
                    if attempt < max_retries - 1:
                        continue

            except Exception as e:
                print(f"Error on attempt {attempt + 1}: {e}")
                if attempt < max_retries - 1:
                    continue

    # åˆå¹¶æ£€æµ‹ç»“æœï¼ˆç®€å•å¤„ç†ï¼šä½¿ç”¨æœ€åä¸€å¸§çš„ç‰©ä½“ï¼‰
    if all_narratives:
        result = {
            "video_narrative": all_narratives,
            "mode": "sliding_window",
            "total_frames": len(image_paths),
            "context_type": "narrative_continuity"
        }
        return result
    else:
        return None


def save_result(result, output_path="output_sliding.json"):
    """ä¿å­˜ç»“æœåˆ° JSON æ–‡ä»¶"""
    try:
        with open(output_path, "w", encoding="utf-8") as f:
            json.dump(result, f, indent=2, ensure_ascii=False)
        print(f"\nâœ“ Result saved to {output_path}")
        return True
    except Exception as e:
        print(f"Error saving result: {e}")
        return False


def print_sliding_summary(result):
    """æ‰“å°æ»‘åŠ¨çª—å£åˆ†ææ‘˜è¦"""
    if not result or "video_narrative" not in result:
        print("\nâŒ No valid result")
        return

    narratives = result["video_narrative"]
    print(f"\n{'='*60}")
    print(f"SLIDING WINDOW ANALYSIS COMPLETE")
    print(f"{'='*60}")
    print(f"Mode: {result.get('mode', 'unknown')}")
    print(f"Frames: {len(narratives)}")
    print(f"{'='*60}\n")

    total_vocab = 0
    for i, entry in enumerate(narratives):
        frame_idx = entry.get("frame_index", i)
        timestamp = entry.get("timestamp", "N/A")
        sentence = entry.get("sentence", "")
        core_word = entry.get("core_word", "")
        vocab_count = entry.get("vocabulary_count", 0)
        total_vocab += vocab_count

        # æ˜¾ç¤ºä¸Šä¸‹æ–‡
        if "context_continuity" in entry:
            prev = entry["context_continuity"].get("previous_sentence", "")
            print(f"ğŸ”— Previous: {prev}")

        print(f"\nFrame {frame_idx} [{timestamp}]")
        print(f"Sentence: {sentence}")
        if core_word:
            print(f"Core Word: {core_word}")
        print(f"Advanced Vocabulary: {vocab_count} word(s)")

    print(f"\n{'='*60}")
    print(f"Total Advanced Vocabulary: {total_vocab} word(s)")
    print(f"{'='*60}\n")


def compare_modes(image_paths):
    """å¯¹æ¯”æ™®é€šæ¨¡å¼å’Œæ»‘åŠ¨çª—å£æ¨¡å¼"""
    print("\n" + "="*60)
    print("ğŸ“Š å¯¹æ¯”æµ‹è¯•ï¼šæ™®é€šæ¨¡å¼ vs æ»‘åŠ¨çª—å£æ¨¡å¼")
    print("="*60)

    # å¯¼å…¥æ™®é€šæ¨¡å¼çš„æµ‹è¯•
    from test import analyze_video_frames_flash as analyze_normal

    print("\n[æ¨¡å¼ 1] æ™®é€šæ¨¡å¼ï¼ˆæ— ä¸Šä¸‹æ–‡ï¼‰")
    print("-" * 60)
    normal_result = analyze_normal(image_paths)
    if normal_result:
        from test import print_summary
        print_summary(normal_result)

    print("\n[æ¨¡å¼ 2] æ»‘åŠ¨çª—å£æ¨¡å¼ï¼ˆå¸¦ä¸Šä¸‹æ–‡ï¼‰")
    print("-" * 60)
    sliding_result = analyze_video_frames_with_sliding_window(image_paths)
    if sliding_result:
        print_sliding_summary(sliding_result)
        save_result(sliding_result, "output_sliding.json")

    # ä¿å­˜å¯¹æ¯”ç»“æœ
    if normal_result and sliding_result:
        comparison = {
            "normal_mode": normal_result,
            "sliding_window_mode": sliding_result,
            "comparison": {
                "total_vocab_normal": sum(n.get("vocabulary_count", 0) for n in normal_result["video_narrative"]),
                "total_vocab_sliding": sum(n.get("vocabulary_count", 0) for n in sliding_result["video_narrative"]),
            }
        }
        with open("comparison.json", "w", encoding="utf-8") as f:
            json.dump(comparison, f, indent=2, ensure_ascii=False)
        print("âœ“ Comparison saved to comparison.json")


def main():
    """ä¸»å‡½æ•°"""
    import sys

    if len(sys.argv) > 1:
        image_paths = sys.argv[1:]
    else:
        # é»˜è®¤ä½¿ç”¨æå–çš„å¸§
        image_paths = [
            "test_output/frames/frame_00.jpg",
            "test_output/frames/frame_01.jpg",
            "test_output/frames/frame_02.jpg",
            "test_output/frames/frame_03.jpg"
        ]

    print(f"Analyzing {len(image_paths)} frame(s) with sliding window...")
    print(f"Frames: {', '.join(image_paths)}")

    # å¯¹æ¯”æµ‹è¯•
    compare_modes(image_paths)


if __name__ == "__main__":
    main()
